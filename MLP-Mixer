from torch import nn
from torch.nn import functional as F
import os  # 导入os模块，用于文件和目录操作
import torch  # 导入torch库，用于构建神经网络
import torchvision  # 导入torchvision库，用于数据集和预训练模型
from torch.utils.data import DataLoader  # 导入DataLoader，用于加载数据
from torchvision import datasets, transforms  # 导入torchvision的datasets和transforms模块，用于数据集和数据转换
import logging  # 导入logging模块，用于日志记录
import random  # 导入random模块，用于固定随机种子
import numpy as np  # 导入numpy模块，用于固定随机种子
import torch.optim as optim  # 导入优化器模块

class MLP(nn.Module):
    def __init__(self, num_features, expansion_factor, dropout):
        super().__init__()
        num_hidden = num_features * expansion_factor
        self.fc1 = nn.Linear(num_features, num_hidden)
        self.dropout1 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(num_hidden, num_features)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        x = self.dropout1(F.gelu(self.fc1(x)))
        x = self.dropout2(self.fc2(x))
        return x


class TokenMixer(nn.Module):
    def __init__(self, num_features, num_patches, expansion_factor, dropout):
        super().__init__()
        self.norm = nn.LayerNorm(num_features)
        self.mlp = MLP(num_patches, expansion_factor, dropout)

    def forward(self, x):
        # x.shape == (batch_size, num_patches, num_features)
        residual = x
        x = self.norm(x)
        x = x.transpose(1, 2)
        # x.shape == (batch_size, num_features, num_patches)
        x = self.mlp(x)
        x = x.transpose(1, 2)
        # x.shape == (batch_size, num_patches, num_features)
        out = x + residual
        return out


class ChannelMixer(nn.Module):
    def __init__(self, num_features, num_patches, expansion_factor, dropout):
        super().__init__()
        self.norm = nn.LayerNorm(num_features)
        self.mlp = MLP(num_features, expansion_factor, dropout)

    def forward(self, x):
        # x.shape == (batch_size, num_patches, num_features)
        residual = x
        x = self.norm(x)
        x = self.mlp(x)
        # x.shape == (batch_size, num_patches, num_features)
        out = x + residual
        return out


class MixerLayer(nn.Module):
    def __init__(self, num_features, num_patches, expansion_factor, dropout):
        super().__init__()
        self.token_mixer = TokenMixer(
            num_patches, num_features, expansion_factor, dropout
        )
        self.channel_mixer = ChannelMixer(
            num_patches, num_features, expansion_factor, dropout
        )

    def forward(self, x):
        # x.shape == (batch_size, num_patches, num_features)
        x = self.token_mixer(x)
        x = self.channel_mixer(x)
        # x.shape == (batch_size, num_patches, num_features)
        return x


def check_sizes(image_size, patch_size):
    sqrt_num_patches, remainder = divmod(image_size, patch_size)
    assert remainder == 0, "`image_size` must be divisible by `patch_size`"
    num_patches = sqrt_num_patches ** 2
    return num_patches


class MLPMixer(nn.Module):
    def __init__(
        self,
        image_size=256,
        patch_size=16,
        in_channels=3,
        num_features=128,
        expansion_factor=2,
        num_layers=8,
        num_classes=10,
        dropout=0.5,
    ):
        num_patches = check_sizes(image_size, patch_size)
        super().__init__()
        # per-patch fully-connected is equivalent to strided conv2d
        self.patcher = nn.Conv2d(
            in_channels, num_features, kernel_size=patch_size, stride=patch_size
        )
        self.mixers = nn.Sequential(
            *[
                MixerLayer(num_patches, num_features, expansion_factor, dropout)
                for _ in range(num_layers)
            ]
        )
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x):
        patches = self.patcher(x)
        batch_size, num_features, _, _ = patches.shape
        patches = patches.permute(0, 2, 3, 1)
        patches = patches.view(batch_size, -1, num_features)
        # patches.shape == (batch_size, num_patches, num_features)
        embedding = self.mixers(patches)
        # embedding.shape == (batch_size, num_patches, num_features)
        embedding = embedding.mean(dim=1)
        logits = self.classifier(embedding)
        return logits

def main():
    # 固定随机种子以保证结果可复现性
    seed = 42
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    # 设置日志记录
    logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # 定义运行设备
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f'使用设备: {device}')

    # 数据预处理
    transform = transforms.Compose([
        transforms.Resize((256, 256)),  # 调整图像大小为 256x256
        transforms.ToTensor(),  # 将图像转换为张量
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化图像
    ])

    # 加载数据集
    train_dataset = datasets.ImageFolder(root=r'D:\resnet\supervised_fault_classification\defect_supervised\glass-insulator\train', transform=transform)
    val_dataset = datasets.ImageFolder(root=r'D:\resnet\supervised_fault_classification\defect_supervised\glass-insulator\val', transform=transform)

    # 创建数据加载器
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # 定义模型
    model = MLPMixer(image_size=256, patch_size=16, in_channels=3, num_features=128, expansion_factor=2, num_layers=8,
                     num_classes=len(train_dataset.classes), dropout=0.5)
    model.to(device)
    logging.info('模型已初始化')

    # 定义损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    logging.info('损失函数和优化器已设置')

    # 训练循环
    num_epochs = 1
    best_val_acc = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        epoch_loss = running_loss / total_train
        epoch_acc = correct_train / total_train

        logging.info(f'Epoch [{epoch + 1}/{num_epochs}], 训练损失: {epoch_loss:.4f}, 训练准确率: {epoch_acc:.4f}')
        print(f'Epoch [{epoch + 1}/{num_epochs}], 训练损失: {epoch_loss:.4f}, 训练准确率: {epoch_acc:.4f}')

        # 验证
        model.eval()
        correct_val = 0
        total_val = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        epoch_val_acc = correct_val / total_val
        logging.info(f'Epoch [{epoch + 1}/{num_epochs}], 验证准确率: {epoch_val_acc:.4f}')
        print(f'Epoch [{epoch + 1}/{num_epochs}], 验证准确率: {epoch_val_acc:.4f}')

        # 保存最佳模型
        if epoch_val_acc > best_val_acc:
            best_val_acc = epoch_val_acc
            checkpoint_path = f'model_epoch_{epoch + 1}_acc_{epoch_val_acc:.4f}.pth'
            torch.save(model.state_dict(), checkpoint_path)
            logging.info(f'保存最佳模型到 {checkpoint_path}')

    print('训练完成')

if __name__ == "__main__":
    main()




